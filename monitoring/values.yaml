
global:
  labels:
    group: com.stakater.platform
    provider: stakater
  hyperkube:
    image: quay.io/coreos/hyperkube
    tag: v1.8.7_coreos.0
    pullPolicy: IfNotPresent
  nodeSelectorZone: us-west-2a

prometheusOperator:
  apiVersion: "monitoring.coreos.com/v1"
  labels:
    version: 0.17.0

  image:
    name: quay.io/coreos/prometheus-operator
    tag: v0.17.0
  deployment:
    replicas: 1
    kubeletService: kube-system/kubelet
    reloaderImage: quay.io/coreos/configmap-reload:v0.0.1
    resources:
      limits:
        cpu: 200m
        memory: 100Mi
      requests:
        cpu: 100m
        memory: 50Mi
  service:
    expose: "true"
    forceSslRedirect: "true"
    ingressClass: internal-ingress
    annotations:

alertmanager:
  labels:
    version: 0.14.0
  image:
    tag: v0.14.0
  replicas: 3
  externalUrl: http://127.0.0.1:9093
  service:
    forceSslRedirect: "true"
    ingressClass: internal-ingress
    annotations:
    expose: "true"
  secrets:
    alertManagerYaml:
      Z2xvYmFsOgoKICByZXNvbHZlX3RpbWVvdXQ6IDVtCgogIHNsYWNrX2FwaV91cmw6ICdodHRwczovL2hvb2tzLnNsYWNrLmNvbS9zZXJ2aWNlcy9UNlA1UFMyVjgvQkFSMDU3U0Y4L1VIdm1ZdUJjVmxNc1RtcGFwV083T3BKMicKCnRlbXBsYXRlczoKCi0gJy9ldGMvYWxlcnRtYW5hZ2VyL3RlbXBsYXRlLyoudG1wbCcKCnJvdXRlOgoKICBncm91cF9ieTogWydhbGVydG5hbWUnLCAnY2x1c3RlcicsICdzZXJ2aWNlJ10KCiAgZ3JvdXBfd2FpdDogMzBzCgogIGdyb3VwX2ludGVydmFsOiA1bQoKICByZXBlYXRfaW50ZXJ2YWw6IDFoCgogIHJlY2VpdmVyOiBkZWZhdWx0LXJlY2VpdmVyCgogIHJvdXRlczoKCiAgLSBtYXRjaDoKCiAgICAgIGFsZXJ0bmFtZTogRGVhZE1hbnNTd2l0Y2gKCiAgICByZWNlaXZlcjogJ251bGwnCgppbmhpYml0X3J1bGVzOgoKLSBzb3VyY2VfbWF0Y2g6CgogICAgc2V2ZXJpdHk6ICdjcml0aWNhbCcKCiAgdGFyZ2V0X21hdGNoOgoKICAgIHNldmVyaXR5OiAnd2FybmluZycKCiAgIyBBcHBseSBpbmhpYml0aW9uIGlmIHRoZSBhbGVydG5hbWUgaXMgdGhlIHNhbWUuCgogIGVxdWFsOiBbJ2FsZXJ0bmFtZScsICdjbHVzdGVyJywgJ3NlcnZpY2UnXQoKcmVjZWl2ZXJzOgoKLSBuYW1lOiAnZGVmYXVsdC1yZWNlaXZlcicKCiAgc2xhY2tfY29uZmlnczoKCiAgLSBjaGFubmVsOiAnI2Rldi1ub3RpZmljYXRpb25zJwoKICAgIHRpdGxlOiAnW3t7IC5TdGF0dXMgfCB0b1VwcGVyIH19e3sgaWYgZXEgLlN0YXR1cyAiZmlyaW5nIiB9fTp7eyAuQWxlcnRzLkZpcmluZyB8IGxlbiB9fXt7IGVuZCB9fV0gUHJvbWV0aGV1cyBFdmVudCBOb3RpZmljYXRpb24nCgogICAgdGl0bGVfbGluazogfAoKICAgICAgaHR0cHM6Ly9hbGVydG1hbmFnZXItbWFpbi5tb25pdG9yaW5nLnN0YWNrYXRvci5jb20KCiAgICB0ZXh0OiA+LQoKICAgICAgICB7eyByYW5nZSAuQWxlcnRzIH19CgogICAgICAgICAgICpBbGVydDoqIHt7IC5Bbm5vdGF0aW9ucy5zdW1tYXJ5IH19IC0gYHt7IC5MYWJlbHMuc2V2ZXJpdHkgfX1gCgogICAgICAgICAgKkRlc2NyaXB0aW9uOioge3sgLkFubm90YXRpb25zLmRlc2NyaXB0aW9uIH19CgogICAgICAgICAgKkdyYXBoOiogPHt7IC5HZW5lcmF0b3JVUkwgfX18OmNoYXJ0X3dpdGhfdXB3YXJkc190cmVuZDo+ICpSdW5ib29rOiogPHt7IC5Bbm5vdGF0aW9ucy5ydW5ib29rIH19fDpzcGlyYWxfbm90ZV9wYWQ6PgoKICAgICAgICAgICpEZXRhaWxzOioKCiAgICAgICAgICB7eyByYW5nZSAuTGFiZWxzLlNvcnRlZFBhaXJzIH19ICp7eyAuTmFtZSB9fToqIGB7eyAuVmFsdWUgfX1gCgogICAgICAgICAge3sgZW5kIH19CgogICAgICAgIHt7IGVuZCB9fQoKICAgIHNlbmRfcmVzb2x2ZWQ6IHRydWUKCi0gbmFtZTogJ251bGwnCgo=

prometheus:
  labels:
    version: 2.2.0-rc.0
  image:
    tag: v2.2.0-rc.0

  service:
    expose: "true"
    forceSslRedirect: "true"
    ingressClass: internal-ingress

  storageclass:
    isDefaultClass: "false"
    annotations:

  prometheusLabel: k8s
  externalUrl: http://127.0.0.1:9090
  replicas: 2
  serviceMonitorExpressions:
    - "{key: k8s-app, operator: Exists}"
  retention: 168h

  resources:
    requests:
      memory: 1Gi

  storage:
    class: ssd
    annotations:
    resources:
      requests:
        storage: 40Gi

  configmap:
    data:
      alertmanager.rules: |+
        groups:
        - name: alertmanager.rules
          rules:
          - alert: AlertmanagerConfigInconsistent
            expr: count_values("config_hash", alertmanager_config_hash) BY (service) / ON(service)
              GROUP_LEFT() label_replace(prometheus_operator_alertmanager_spec_replicas, "service",
              "alertmanager-$1", "alertmanager", "(.*)") != 1
            for: 5m
            labels:
              severity: critical
            annotations:
              description: The configuration of the instances of the Alertmanager cluster
                `{{$labels.service}}` are out of sync.
              summary: Alertmanager configurations are inconsistent
          - alert: AlertmanagerDownOrMissing
            expr: label_replace(prometheus_operator_alertmanager_spec_replicas, "job", "alertmanager-$1",
              "alertmanager", "(.*)") / ON(job) GROUP_RIGHT() sum(up) BY (job) != 1
            for: 5m
            labels:
              severity: warning
            annotations:
              description: An unexpected number of Alertmanagers are scraped or Alertmanagers
                disappeared from discovery.
              summary: Alertmanager down or not discovered
          - alert: FailedReload
            expr: alertmanager_config_last_reload_successful == 0
            for: 10m
            labels:
              severity: warning
            annotations:
              description: Reloading Alertmanager's configuration has failed for {{ $labels.namespace
                }}/{{ $labels.pod}}.
              summary: Alertmanager configuration reload has failed
      etcd3.rules: |+
        groups:
        - name: etcd.rules
          rules:
          - alert: InsufficientMembers
            expr: count(up{job="etcd"} == 0) > (count(up{job="etcd"}) / 2 - 1)
            for: 3m
            labels:
              severity: critical
            annotations:
              description: If one more etcd member goes down the cluster will be unavailable
              summary: etcd cluster insufficient members
          - alert: NoLeader
            expr: etcd_server_has_leader{job="etcd"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              description: etcd member {{ $labels.instance }} has no leader
              summary: etcd member has no leader
          - alert: HighNumberOfLeaderChanges
            expr: increase(etcd_server_leader_changes_seen_total{job="etcd"}[1h]) > 3
            labels:
              severity: warning
            annotations:
              description: etcd instance {{ $labels.instance }} has seen {{ $value }} leader
                changes within the last hour
              summary: a high number of leader changes within the etcd cluster are happening
          - alert: HighNumberOfFailedGRPCRequests
            expr: sum(rate(etcd_grpc_requests_failed_total{job="etcd"}[5m])) BY (grpc_method)
              / sum(rate(etcd_grpc_total{job="etcd"}[5m])) BY (grpc_method) > 0.01
            for: 10m
            labels:
              severity: warning
            annotations:
              description: '{{ $value }}% of requests for {{ $labels.grpc_method }} failed
                on etcd instance {{ $labels.instance }}'
              summary: a high number of gRPC requests are failing
          - alert: HighNumberOfFailedGRPCRequests
            expr: sum(rate(etcd_grpc_requests_failed_total{job="etcd"}[5m])) BY (grpc_method)
              / sum(rate(etcd_grpc_total{job="etcd"}[5m])) BY (grpc_method) > 0.05
            for: 5m
            labels:
              severity: critical
            annotations:
              description: '{{ $value }}% of requests for {{ $labels.grpc_method }} failed
                on etcd instance {{ $labels.instance }}'
              summary: a high number of gRPC requests are failing
          - alert: GRPCRequestsSlow
            expr: histogram_quantile(0.99, rate(etcd_grpc_unary_requests_duration_seconds_bucket[5m]))
              > 0.15
            for: 10m
            labels:
              severity: critical
            annotations:
              description: on etcd instance {{ $labels.instance }} gRPC requests to {{ $labels.grpc_method
                }} are slow
              summary: slow gRPC requests
          - alert: HighNumberOfFailedHTTPRequests
            expr: sum(rate(etcd_http_failed_total{job="etcd"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job="etcd"}[5m]))
              BY (method) > 0.01
            for: 10m
            labels:
              severity: warning
            annotations:
              description: '{{ $value }}% of requests for {{ $labels.method }} failed on etcd
                instance {{ $labels.instance }}'
              summary: a high number of HTTP requests are failing
          - alert: HighNumberOfFailedHTTPRequests
            expr: sum(rate(etcd_http_failed_total{job="etcd"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job="etcd"}[5m]))
              BY (method) > 0.05
            for: 5m
            labels:
              severity: critical
            annotations:
              description: '{{ $value }}% of requests for {{ $labels.method }} failed on etcd
                instance {{ $labels.instance }}'
              summary: a high number of HTTP requests are failing
          - alert: HTTPRequestsSlow
            expr: histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[5m]))
              > 0.15
            for: 10m
            labels:
              severity: warning
            annotations:
              description: on etcd instance {{ $labels.instance }} HTTP requests to {{ $labels.method
                }} are slow
              summary: slow HTTP requests
          - alert: EtcdMemberCommunicationSlow
            expr: histogram_quantile(0.99, rate(etcd_network_member_round_trip_time_seconds_bucket[5m]))
              > 0.15
            for: 10m
            labels:
              severity: warning
            annotations:
              description: etcd instance {{ $labels.instance }} member communication with
                {{ $labels.To }} is slow
              summary: etcd member communication is slow
          - alert: HighNumberOfFailedProposals
            expr: increase(etcd_server_proposals_failed_total{job="etcd"}[1h]) > 5
            labels:
              severity: warning
            annotations:
              description: etcd instance {{ $labels.instance }} has seen {{ $value }} proposal
                failures within the last hour
              summary: a high number of proposals within the etcd cluster are failing
          - alert: HighFsyncDurations
            expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m]))
              > 0.5
            for: 10m
            labels:
              severity: warning
            annotations:
              description: etcd instance {{ $labels.instance }} fync durations are high
              summary: high fsync durations
          - alert: HighCommitDurations
            expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m]))
              > 0.25
            for: 10m
            labels:
              severity: warning
            annotations:
              description: etcd instance {{ $labels.instance }} commit durations are high
              summary: high commit durations
      general.rules: |+
        groups:
        - name: pods.rules
          rules:
          - alert: PodsDown
            expr: kube_pod_info{created_by_kind!="Job"} == 1 and ON(pod) kube_pod_status_phase{phase!="Pending"} == 1 and ON(pod) kube_pod_status_ready{condition="false"} == 1
            for: 1s
            labels:
              severity: critical
            annotations:
              description: '{{ $labels.pod }} from {{ $labels.namespace }} is red.'
              summary: Pods are down
          - alert: JobsFailed
            expr: kube_pod_info{created_by_kind="Job"} == 1 and ON(pod) (kube_pod_status_phase{phase="Failed"} == 1 or kube_pod_status_phase{phase="Unknown"} == 1)
            for: 1s
            labels:
              severity: critical
            annotations:
              description: '{{ $labels.pod }} from {{ $labels.namespace }} is red.'
              summary: Jobs failed
        - name: general.rules
          rules:
          - alert: TargetDown
            expr: 100 * (count(up == 0) BY (job) / count(up) BY (job)) > 10
            for: 10m
            labels:
              severity: warning
            annotations:
              description: '{{ $value }}% or more of {{ $labels.job }} targets are down.'
              summary: Targets are down
          - alert: DeadMansSwitch
            expr: vector(1)
            labels:
              severity: none
            annotations:
              description: This is a DeadMansSwitch meant to ensure that the entire Alerting
                pipeline is functional.
              summary: Alerting DeadMansSwitch
          - alert: TooManyOpenFileDescriptors
            expr: 100 * (process_open_fds / process_max_fds) > 95
            for: 10m
            labels:
              severity: critical
            annotations:
              description: '{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} ({{
                $labels.instance }}) is using {{ $value }}% of the available file/socket descriptors.'
              summary: too many open file descriptors
          - record: instance:fd_utilization
            expr: process_open_fds / process_max_fds
          - alert: FdExhaustionClose
            expr: predict_linear(instance:fd_utilization[1h], 3600 * 4) > 1
            for: 10m
            labels:
              severity: warning
            annotations:
              description: '{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} ({{
                $labels.instance }}) instance will exhaust in file/socket descriptors soon'
              summary: file descriptors soon exhausted
          - alert: FdExhaustionClose
            expr: predict_linear(instance:fd_utilization[10m], 3600) > 1
            for: 10m
            labels:
              severity: critical
            annotations:
              description: '{{ $labels.job }}: {{ $labels.namespace }}/{{ $labels.pod }} ({{
                $labels.instance }}) instance will exhaust in file/socket descriptors soon'
              summary: file descriptors soon exhausted
      job.rules: |+
        groups:
        - name: job.rules
          rules:
          - alert: CronJobRunning
            expr: time() -kube_cronjob_next_schedule_time > 3600
            for: 1h
            labels:
              severity: warning
            annotations:
              description: CronJob {{$labels.namespaces}}/{{$labels.cronjob}} is taking more than 1h to complete
              summary: CronJob didn't finish after 1h

          - alert: JobCompletion
            expr: kube_job_spec_completions - kube_job_status_succeeded  > 0
            for: 1h
            labels:
              severity: warning
            annotations:
              description: Job completion is taking more than 1h to complete
                cronjob {{$labels.namespaces}}/{{$labels.job}}
              summary: Job {{$labels.job}} didn't finish to complete after 1h

          - alert: JobFailed
            expr: kube_job_status_failed  > 0
            for: 1h
            labels:
              severity: warning
            annotations:
              description: Job {{$labels.namespaces}}/{{$labels.job}} failed to complete
              summary: Job failed
      kube-apiserver.rules: |+
        groups:
        - name: kube-apiserver.rules
          rules:
          - alert: K8SApiServerLatency
            expr: histogram_quantile(0.99, sum(apiserver_request_latencies_bucket{subresource!="log",verb!~"CONNECT|WATCHLIST|WATCH|PROXY"})
              WITHOUT (instance, resource)) / 1e+06 > 1
            for: 10m
            labels:
              severity: warning
            annotations:
              description: 99th percentile Latency for {{ $labels.verb }} requests to the
                kube-apiserver is higher than 1s.
              summary: Kubernetes apiserver latency is high
      kubelet.rules: |+
        groups:
        - name: kubelet.rules
          rules:
          - alert: K8SNodeNotReady
            expr: kube_node_status_condition{condition="Ready",status="true"} == 0
            for: 1h
            labels:
              severity: warning
            annotations:
              description: The Kubelet on {{ $labels.node }} has not checked in with the API,
                or has set itself to NotReady, for more than an hour
              summary: Node status is NotReady
          - alert: K8SManyNodesNotReady
            expr: count(kube_node_status_condition{condition="Ready",status="true"} == 0)
              > 1 and (count(kube_node_status_condition{condition="Ready",status="true"} ==
              0) / count(kube_node_status_condition{condition="Ready",status="true"})) > 0.2
            for: 1m
            labels:
              severity: critical
            annotations:
              description: '{{ $value }} Kubernetes nodes (more than 10% are in the NotReady
                state).'
              summary: Many Kubernetes nodes are Not Ready
          - alert: K8SKubeletDown
            expr: count(up{job="kubelet"} == 0) / count(up{job="kubelet"}) > 0.03
            for: 1h
            labels:
              severity: warning
            annotations:
              description: Prometheus failed to scrape {{ $value }}% of kubelets.
              summary: Many Kubelets cannot be scraped
          - alert: K8SKubeletDown
            expr: absent(up{job="kubelet"} == 1) or count(up{job="kubelet"} == 0) / count(up{job="kubelet"})
              > 0.1
            for: 1h
            labels:
              severity: critical
            annotations:
              description: Prometheus failed to scrape {{ $value }}% of kubelets, or all Kubelets
                have disappeared from service discovery.
              summary: Many Kubelets cannot be scraped
          - alert: K8SKubeletTooManyPods
            expr: kubelet_running_pod_count > 100
            labels:
              severity: warning
            annotations:
              description: Kubelet {{$labels.instance}} is running {{$value}} pods, close
                to the limit of 110
              summary: Kubelet is close to pod limit
      kubernetes.rules: |+
        groups:
        - name: kubernetes.rules
          rules:
          - record: cluster_namespace_controller_pod_container:spec_memory_limit_bytes
            expr: sum(label_replace(container_spec_memory_limit_bytes{container_name!=""},
              "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace,
              controller, pod_name, container_name)
          - record: cluster_namespace_controller_pod_container:spec_cpu_shares
            expr: sum(label_replace(container_spec_cpu_shares{container_name!=""}, "controller",
              "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name,
              container_name)
          - record: cluster_namespace_controller_pod_container:cpu_usage:rate
            expr: sum(label_replace(irate(container_cpu_usage_seconds_total{container_name!=""}[5m]),
              "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace,
              controller, pod_name, container_name)
          - record: cluster_namespace_controller_pod_container:memory_usage:bytes
            expr: sum(label_replace(container_memory_usage_bytes{container_name!=""}, "controller",
              "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name,
              container_name)
          - record: cluster_namespace_controller_pod_container:memory_working_set:bytes
            expr: sum(label_replace(container_memory_working_set_bytes{container_name!=""},
              "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace,
              controller, pod_name, container_name)
          - record: cluster_namespace_controller_pod_container:memory_rss:bytes
            expr: sum(label_replace(container_memory_rss{container_name!=""}, "controller",
              "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name,
              container_name)
          - record: cluster_namespace_controller_pod_container:memory_cache:bytes
            expr: sum(label_replace(container_memory_cache{container_name!=""}, "controller",
              "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name,
              container_name)
          - record: cluster_namespace_controller_pod_container:disk_usage:bytes
            expr: sum(label_replace(container_disk_usage_bytes{container_name!=""}, "controller",
              "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace, controller, pod_name,
              container_name)
          - record: cluster_namespace_controller_pod_container:memory_pagefaults:rate
            expr: sum(label_replace(irate(container_memory_failures_total{container_name!=""}[5m]),
              "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace,
              controller, pod_name, container_name, scope, type)
          - record: cluster_namespace_controller_pod_container:memory_oom:rate
            expr: sum(label_replace(irate(container_memory_failcnt{container_name!=""}[5m]),
              "controller", "$1", "pod_name", "^(.*)-[a-z0-9]+")) BY (cluster, namespace,
              controller, pod_name, container_name, scope, type)
          - record: cluster:memory_allocation:percent
            expr: 100 * sum(container_spec_memory_limit_bytes{pod_name!=""}) BY (cluster)
              / sum(machine_memory_bytes) BY (cluster)
          - record: cluster:memory_used:percent
            expr: 100 * sum(container_memory_usage_bytes{pod_name!=""}) BY (cluster) / sum(machine_memory_bytes)
              BY (cluster)
          - record: cluster:cpu_allocation:percent
            expr: 100 * sum(container_spec_cpu_shares{pod_name!=""}) BY (cluster) / sum(container_spec_cpu_shares{id="/"}
              * ON(cluster, instance) machine_cpu_cores) BY (cluster)
          - record: cluster:node_cpu_use:percent
            expr: 100 * sum(rate(node_cpu{mode!="idle"}[5m])) BY (cluster) / sum(machine_cpu_cores)
              BY (cluster)
          - record: cluster_resource_verb:apiserver_latency:quantile_seconds
            expr: histogram_quantile(0.99, sum(apiserver_request_latencies_bucket) BY (le,
              cluster, job, resource, verb)) / 1e+06
            labels:
              quantile: "0.99"
          - record: cluster_resource_verb:apiserver_latency:quantile_seconds
            expr: histogram_quantile(0.9, sum(apiserver_request_latencies_bucket) BY (le,
              cluster, job, resource, verb)) / 1e+06
            labels:
              quantile: "0.9"
          - record: cluster_resource_verb:apiserver_latency:quantile_seconds
            expr: histogram_quantile(0.5, sum(apiserver_request_latencies_bucket) BY (le,
              cluster, job, resource, verb)) / 1e+06
            labels:
              quantile: "0.5"
          - record: cluster:scheduler_e2e_scheduling_latency:quantile_seconds
            expr: histogram_quantile(0.99, sum(scheduler_e2e_scheduling_latency_microseconds_bucket)
              BY (le, cluster)) / 1e+06
            labels:
              quantile: "0.99"
          - record: cluster:scheduler_e2e_scheduling_latency:quantile_seconds
            expr: histogram_quantile(0.9, sum(scheduler_e2e_scheduling_latency_microseconds_bucket)
              BY (le, cluster)) / 1e+06
            labels:
              quantile: "0.9"
          - record: cluster:scheduler_e2e_scheduling_latency:quantile_seconds
            expr: histogram_quantile(0.5, sum(scheduler_e2e_scheduling_latency_microseconds_bucket)
              BY (le, cluster)) / 1e+06
            labels:
              quantile: "0.5"
          - record: cluster:scheduler_scheduling_algorithm_latency:quantile_seconds
            expr: histogram_quantile(0.99, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket)
              BY (le, cluster)) / 1e+06
            labels:
              quantile: "0.99"
          - record: cluster:scheduler_scheduling_algorithm_latency:quantile_seconds
            expr: histogram_quantile(0.9, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket)
              BY (le, cluster)) / 1e+06
            labels:
              quantile: "0.9"
          - record: cluster:scheduler_scheduling_algorithm_latency:quantile_seconds
            expr: histogram_quantile(0.5, sum(scheduler_scheduling_algorithm_latency_microseconds_bucket)
              BY (le, cluster)) / 1e+06
            labels:
              quantile: "0.5"
          - record: cluster:scheduler_binding_latency:quantile_seconds
            expr: histogram_quantile(0.99, sum(scheduler_binding_latency_microseconds_bucket)
              BY (le, cluster)) / 1e+06
            labels:
              quantile: "0.99"
          - record: cluster:scheduler_binding_latency:quantile_seconds
            expr: histogram_quantile(0.9, sum(scheduler_binding_latency_microseconds_bucket)
              BY (le, cluster)) / 1e+06
            labels:
              quantile: "0.9"
          - record: cluster:scheduler_binding_latency:quantile_seconds
            expr: histogram_quantile(0.5, sum(scheduler_binding_latency_microseconds_bucket)
              BY (le, cluster)) / 1e+06
            labels:
              quantile: "0.5"
      kube-state-metrics.rules: |+
        groups:
        - name: kube-state-metrics.rules
          rules:
          - alert: DeploymentGenerationMismatch
            expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
            for: 15m
            labels:
              severity: warning
            annotations:
              description: Observed deployment generation does not match expected one for
                deployment {{$labels.namespaces}}/{{$labels.deployment}}
              summary: Deployment is outdated
          - alert: DeploymentReplicasNotUpdated
            expr: ((kube_deployment_status_replicas_updated != kube_deployment_spec_replicas)
              or (kube_deployment_status_replicas_available != kube_deployment_spec_replicas))
              unless (kube_deployment_spec_paused == 1)
            for: 15m
            labels:
              severity: warning
            annotations:
              description: Replicas are not updated and available for deployment {{$labels.namespaces}}/{{$labels.deployment}}
              summary: Deployment replicas are outdated
          - alert: DaemonSetRolloutStuck
            expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled
              * 100 < 100
            for: 15m
            labels:
              severity: warning
            annotations:
              description: Only {{$value}}% of desired pods scheduled and ready for daemon
                set {{$labels.namespaces}}/{{$labels.daemonset}}
              summary: DaemonSet is missing pods
          - alert: K8SDaemonSetsNotScheduled
            expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled
              > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              description: A number of daemonsets are not scheduled.
              summary: Daemonsets are not scheduled correctly
          - alert: DaemonSetsMissScheduled
            expr: kube_daemonset_status_number_misscheduled > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              description: A number of daemonsets are running where they are not supposed
                to run.
              summary: Daemonsets are not scheduled correctly
          - alert: PodFrequentlyRestarting
            expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
            for: 10m
            labels:
              severity: warning
            annotations:
              description: Pod {{$labels.namespaces}}/{{$labels.pod}} is was restarted {{$value}}
                times within the last hour
              summary: Pod is restarting frequently
      node.rules: |+
        groups:
        - name: node.rules
          rules:
          - alert: NodeExporterDown
            expr: absent(up{job="node-exporter"} == 1)
            for: 10m
            labels:
              severity: warning
            annotations:
              description: Prometheus could not scrape a node-exporter for more than 10m,
                or node-exporters have disappeared from discovery.
              summary: node-exporter cannot be scraped
          - alert: K8SNodeOutOfDisk
            expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
            labels:
              service: k8s
              severity: critical
            annotations:
              description: '{{ $labels.node }} has run out of disk space.'
              summary: Node ran out of disk space.
          - alert: K8SNodeMemoryPressure
            expr: kube_node_status_condition{condition="MemoryPressure",status="true"} ==
              1
            labels:
              service: k8s
              severity: warning
            annotations:
              description: '{{ $labels.node }} is under memory pressure.'
              summary: Node is under memory pressure.
          - alert: K8SNodeDiskPressure
            expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
            labels:
              service: k8s
              severity: warning
            annotations:
              description: '{{ $labels.node }} is under disk pressure.'
              summary: Node is under disk pressure.
          - alert: NodeCPUUsage
            expr: (100 - (avg by (instance) (irate(node_cpu{job="node-exporter",mode="idle"}[5m])) * 100)) > 90
            for: 30m
            labels:
              severity: warning
            annotations:
              summary: "{{$labels.instance}}: High CPU usage detected"
              description: "{{$labels.instance}}: CPU usage is above 90% (current value is: {{ $value }})"
          - alert: NodeMemoryUsage
            expr: (((node_memory_MemTotal-node_memory_MemFree-node_memory_Cached)/(node_memory_MemTotal)*100)) > 90
            for: 30m
            labels:
              severity: warning
            annotations:
              summary: "{{$labels.instance}}: High memory usage detected"
              description: "{{$labels.instance}}: Memory usage is above 90% (current value is: {{ $value }})"
      prometheus.rules: |+
        groups:
        - name: prometheus.rules
          rules:
          - alert: FailedReload
            expr: prometheus_config_last_reload_successful == 0
            for: 10m
            labels:
              severity: warning
            annotations:
              description: Reloading Prometheus' configuration has failed for {{ $labels.namespace
                }}/{{ $labels.pod}}.
              summary: Prometheus configuration reload has failed


serviceMonitor:
  prometheus:
    interval: 30s
    namespaceSelector:
    - monitoring
  nodeExporter:
    interval: 30s
    namespaceSelector:
    - monitoring
  kubelet:
    interval: 30s
    namespaceSelector:
    - kube-system
  kubeStateMetrics:
    interval: 30s
    namespaceSelector:
    - monitoring
  kubeDNS:
    interval: 30s
    namespaceSelector:
    - kube-system
  alertmanager:
    interval: 30s
    namespaceSelector:
    - monitoring
  kubeApiserver:
    interval: 30s
    namespaceSelector:
    - default
  kubeControllerManager:
    interval: 30s
    namespaceSelector:
    - kube-system
  kubeScheduler:
    interval: 30s
    namespaceSelector:
    - kube-system
